{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6095fa",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2a017",
   "metadata": {},
   "source": [
    "## Load libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4566299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372de540",
   "metadata": {},
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[-1] # Only keep the last output in the sequence\n",
    "        out = self.fc(lstm_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12bcfed",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e87eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BikeDataset(Dataset):\n",
    "    def __init__(self, csv_file, seq_length):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        beg_idx, end_idx = index, index+self.seq_length\n",
    "        input_features = torch.tensor(self.df.iloc[beg_idx:end_idx,:-1].values,dtype=torch.float32)\n",
    "        target_label = torch.tensor(self.df.iloc[end_idx,-1],dtype=torch.float32)\n",
    "        return input_features, target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'data/Bike-Sharing-Dataset/hour.csv'\n",
    "\n",
    "seq_length = 1\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1-train_ratio\n",
    "batch_size = 1\n",
    "\n",
    "dataset = BikeDataset(csv_file, seq_length)\n",
    "\n",
    "num_samples = len(dataset)\n",
    "num_train_samples = int(train_ratio * num_samples)\n",
    "num_test_samples = num_samples - num_train_samples\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [num_train_samples, num_test_samples])\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # batch_size 1\n",
    "test_dl = DataLoader(test_dataset, batch_size=num_test_samples, shuffle=True) # batch_size ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb1bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over the data loader to access batches of data\n",
    "for batch in train_dl:\n",
    "    input_features, target_label = batch\n",
    "    #print('Input Features:', input_features)\n",
    "    #print('Target Label:', target_label)\n",
    "    print(input_features.transpose(0,1).shape)\n",
    "    print(target_label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee1aad9",
   "metadata": {},
   "source": [
    "## Fit models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7c12e",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0c5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 11\n",
    "hidden_dim = 5\n",
    "output_dim = 1\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82780645",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm # status bar\n",
    "\n",
    "num_epochs=10\n",
    "\n",
    "num_train_samples=len(train_dl)\n",
    "num_val_samples=len(test_dl)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch_idx, (samples, labels) in enumerate(tqdm(train_dl)):\n",
    "\n",
    "        # forward pass\n",
    "        prediction = model(samples.transpose(0,1))\n",
    "        loss = loss_fn(prediction, labels.view(-1,1))\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # training stats\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (samples, labels) in enumerate(test_dl):\n",
    "\n",
    "            # Forward pass\n",
    "            prediction = model(samples.transpose(0,1))\n",
    "            loss = loss_fn(prediction, labels.view(-1,1))\n",
    "\n",
    "            # validation stats\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # lr scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # show training stats for the epoch\n",
    "    print('Epoch [{}/{}]\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}'\n",
    "          .format(epoch + 1, num_epochs, train_loss/num_train_samples, val_loss/num_val_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb747cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23197bc0",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4498788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "# seq_len, train_loss, val_loss\n",
    "# 1      , 14563     , 14894\n",
    "# 2      , 8508      , 8249\n",
    "# 4      , 8027      , 7617\n",
    "# 8      , 7534      , 7392\n",
    "# 12     , 9304      , 9353\n",
    "# 24     , 15075     , 14823\n",
    "# 48     , 12119     , 12023\n",
    "\n",
    "# Sample data\n",
    "x = np.array([1, 2, 4, 8, 12, 24, 48])\n",
    "y = np.array([14894,8249,7617,7392,9353,14823,12023])\n",
    "\n",
    "# Create a smoothing spline\n",
    "spl = UnivariateSpline(x, y)\n",
    "\n",
    "# Generate more points for the plot\n",
    "x_smooth = np.linspace(x.min(), x.max(), 1000)\n",
    "y_smooth = spl(x_smooth)\n",
    "\n",
    "# Create a line plot\n",
    "plt.plot(x_smooth, y_smooth)\n",
    "plt.xlabel('seq_length'); plt.ylabel('val loss')\n",
    "plt.title('seq_length vs val loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "vis_dl = DataLoader(dataset, batch_size=len(dataset), shuffle=True) # batch_size ALL\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for batch in vis_dl:\n",
    "        samples, labels = batch\n",
    "        \n",
    "        lstm_out, (hn, cn) = model.lstm(samples.transpose(0,1))\n",
    "        \n",
    "        prediction = model(samples.transpose(0,1))\n",
    "        loss = loss_fn(prediction, labels.view(-1,1))\n",
    "        \n",
    "    #print(samples.transpose(0,1).shape)\n",
    "    print(prediction.squeeze())\n",
    "    print(labels.view(-1,1).squeeze())\n",
    "    #print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(hidden_dim == 2):\n",
    "    # hidden\n",
    "    plt.scatter(hn[:,:,0].tolist(),hn[:,:,1].tolist(),c=labels.tolist())\n",
    "    plt.title('Scatter plot of hidden state')\n",
    "    # cell\n",
    "#     plt.scatter(cn[:,:,0].tolist(),cn[:,:,1].tolist(),c=labels.tolist())\n",
    "#     plt.title('Scatter plot of cell state')\n",
    "else:\n",
    "    # Apply t-SNE for dimension reduction to 2-D\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    coord_2d = tsne.fit_transform(cn.squeeze().numpy())\n",
    "    plt.scatter(coord_2d[:,0],coord_2d[:,1],c=labels.tolist())\n",
    "    plt.title('t-SNE of cell state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f6e812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870a51f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
