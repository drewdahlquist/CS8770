{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c34e8d",
   "metadata": {},
   "source": [
    "# CS 8770 Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b807eb",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef965873",
   "metadata": {},
   "source": [
    "### Load libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3360a734",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor())\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dab3e1",
   "metadata": {},
   "source": [
    "### Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c90ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    # H: list of hidden layer dims\n",
    "    # phi: non-linearity to use\n",
    "    # n_classes: num of classes to pred\n",
    "    def __init__(self, H, phi=nn.ReLU(), n_classes=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential()\n",
    "        # create hidden layers based off input list H\n",
    "        H.insert(0,28*28) # input layer\n",
    "        [self.layers.append(nn.Linear(h,l)).append(phi) for h, l in zip(H,H[1:])] # hidden layers\n",
    "        self.layers.append(nn.Linear(H[-1],n_classes)) # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) # 28x28 -> 1x784\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cbce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RBFNet(nn.Module):\n",
    "    \n",
    "#     # clusters: precomputed K-means clusters for init\n",
    "#     # n_classes: num of classes to pred\n",
    "#     def __init__(self, clusters, n_classes=10):\n",
    "#         super(RBFNet, self).__init__()\n",
    "#         self.K = clusters.shape[0] # number of clusters/RBFs\n",
    "# #         self.nout = 10\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.sig = nn.Parameter(torch.ones(self.K,dtype=torch.float64))\n",
    "#         self.mu = nn.Parameter(torch.from_numpy(clusters))\n",
    "# #         self.w = nn.Parameter(torch.rand(self.K), requires_grad=True)\n",
    "# #         self.b = nn.Parameter(torch.rand(self.nout), requires_grad=True)\n",
    "#         self.lin = nn.Linear(self.K, n_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         N = x.shape[0] # number of samples\n",
    "#         x = self.flatten(x) # 28x28 -> 1x784\n",
    "#         rbf_out = torch.zeros(N, self.K, dtype=torch.float32)\n",
    "#         # RBF pass\n",
    "#         for i in range(N):\n",
    "#             for j in range(self.K):\n",
    "#                 top = (x[i,:]-self.mu[j:]).pow(2).sum().sqrt()\n",
    "#                 rbf_out[i,j] = torch.exp((-0.5)*(top.pow(2) / self.sig[j].pow(2)))\n",
    "#         # Perceptron\n",
    "#         # TODO: this should be softmax for multi-class?\n",
    "# #         y_pred = torch.sigmoid(torch.mv(rbf_out,self.w)+torch.tensor([self.b]*N, dtype-torch.float64))\n",
    "#         y_pred = self.lin(rbf_out)\n",
    "#         return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d13fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single RBF Neuron\n",
    "class RBFNeuron(nn.Module):\n",
    "\n",
    "    # mu: RBF mu vector\n",
    "    # sig: RBF sigma\n",
    "    def __init__(self, mu, sig):\n",
    "        super(RBFNeuron, self).__init__()\n",
    "        self.mu = mu.clone().detach()\n",
    "        self.sig = sig.clone().detach()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        top = (x-self.mu).pow(2).sum(1).sqrt() # ||x-mu||\n",
    "        return torch.exp((-0.5)*(top / self.sig.pow(2)))\n",
    "\n",
    "# Layer of RBF Neurons\n",
    "class RBFLayer(nn.Module):\n",
    "\n",
    "    # nin: input dim\n",
    "    # nout: output dim\n",
    "    # mus: list of mean vectors for RBF neurons\n",
    "    # sigs: list of sigmas for RBF neurons\n",
    "    def __init__(self, nin, nout, mus, sigs):\n",
    "        super(RBFLayer, self).__init__()\n",
    "        self.neurons = nn.ModuleList([RBFNeuron(mus[i],sigs[i]) for i in range(nout)])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return torch.tensor([f(x).numpy() for f in self.neurons], dtype=torch.float32).transpose(0,1)\n",
    "\n",
    "# Full RBF Network\n",
    "class RBFNet(nn.Module):\n",
    "\n",
    "    # clusters: precomputed K-means clusters for init\n",
    "    # n_classes: num of classes to pred\n",
    "    def __init__(self, clusters, n_classes=10):\n",
    "        super(RBFNet, self).__init__()\n",
    "        self.K = clusters.shape[0] # number of RBFs\n",
    "        # should probably be passing in mus, sigs...\n",
    "        self.mus = nn.Parameter(torch.from_numpy(clusters), requires_grad=False)\n",
    "        self.sigs = nn.Parameter(torch.ones(self.K, dtype=torch.float64, requires_grad=False)*1e-5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            RBFLayer(28*28, self.K, self.mus, self.sigs),\n",
    "            nn.Linear(self.K, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) # 28x28 -> 1x784\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, size, n_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.extract = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=size, out_channels=4,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(in_channels=4, out_channels=4,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.decimate = nn.Sequential(\n",
    "            nn.Linear(7*7*4, 12),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(12, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.extract(x)\n",
    "        x = self.flatten(x)\n",
    "        y_pred = self.decimate(x)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98222284",
   "metadata": {},
   "source": [
    "### Train (& validation / test?) loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b26434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm # status bar\n",
    "\n",
    "def train(model, data, loss_fn, optimizer, epochs=5):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        epoch_loss = []\n",
    "\n",
    "        for batch, (samples, labels) in enumerate(tqdm(data)):\n",
    "\n",
    "            # we need to convert these into tensors\n",
    "            #samples = samples.type('torch.FloatTensor')\n",
    "            #labels = labels.type('torch.LongTensor')\n",
    "\n",
    "            # forward pass\n",
    "            prediction = model(samples)\n",
    "            loss = loss_fn(prediction, labels)\n",
    "\n",
    "            # backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # record loss\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "        # keep track of loss over our batches\n",
    "        #epoch_loss = statistics.mean(epoch_loss)\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b93356",
   "metadata": {},
   "source": [
    "### Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6613dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aac488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.3\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6408d",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa72cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP([128,128], nn.ReLU(), n_classes=10)\n",
    "\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b583dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(mlp_model, train_dl, loss_fn, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4b8f7",
   "metadata": {},
   "source": [
    "### RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(1000, init='k-means++', n_init='auto', random_state=0)\n",
    "kmeans.fit(train_data.data.flatten(1))\n",
    "#klabels = kmeans.predict(train_data.data.flatten(1))\n",
    "clusters = kmeans.cluster_centers_.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf4e7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_model = RBFNet(clusters, n_classes=10)\n",
    "\n",
    "optimizer = optim.Adam(rbf_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca48719",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(rbf_model, train_dl, loss_fn, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e5682",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN(1, n_classes=10)\n",
    "\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5524ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(cnn_model, train_dl, loss_fn, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea7212",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f509d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "# resub because we are loading our MNIST training data set\n",
    "test_dl_2 = DataLoader(dataset=test_data, shuffle=True, batch_size=1)\n",
    "\n",
    "model = mlp_model\n",
    "\n",
    "confusion_mat = torch.zeros((10,10))\n",
    "for sample, label in tqdm(test_dl_2):\n",
    "    \n",
    "    label = int(label.numpy())\n",
    "\n",
    "    prediction = model(sample)\n",
    "    # take the largest output and return integer of which it was (make a classification decision)\n",
    "    prediction = int(torch.argmax(prediction).numpy())\n",
    "    \n",
    "    confusion_mat[label,prediction] += 1\n",
    "    \n",
    "df_cm = pd.DataFrame(np.asarray(ConfusionMatrix), index = [i for i in \"0123456789\"],\n",
    "                  columns = [i for i in \"0123456789\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
